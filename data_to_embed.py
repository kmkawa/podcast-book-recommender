# -*- coding: utf-8 -*-
"""data_to_embed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y8Gz5-LbVBPeT6pLZo3Bi4PD4_OGAQMY

Podcast Data --> embeddings

Sorry this isn't super clean, I can clean more for the final project. We will probably use it a little differently because I was starting at a .json and then going to .csv but you'll just go from .csv to .csv so it might be simpler. You just have to load the model and then run it. I didn't play with any parameters, so if you think something should change, that is totally fine, just let me know. You probably have a bit more/longer descriptions than me, so it might take a little longer to run, but shouldn't be crazy.
"""

!pip install transformers torch pandas numpy

from transformers import BertTokenizer, BertModel
import torch
import numpy as np

# Load model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

#takes in the text and returns the embedings
def bert_embed(texts, batch_size=16, max_length=256):
    #Generate BERT embeddings for a list of texts using mean pooling.

    model.eval()
    embeddings = []

    with torch.no_grad():
        #for each text
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
            outputs = model(**inputs)
            last_hidden_state = outputs.last_hidden_state
            mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()

            # Mean pooling
            pooled_embedding = torch.sum(last_hidden_state * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)
            embeddings.append(pooled_embedding.cpu().numpy())

    return np.vstack(embeddings)

from google.colab import drive

drive.mount('/content/drive')

import json

# Origiinally in .json, but changing to .csv
json_path = "/content/drive/My Drive/DIS Work/Core Course Work/podcast_data/spotify_podcast_data_cleaned.json"
with open(json_path, "r") as f:
    data = json.load(f)
# getting titles & descriptions
titles = list(data.keys())
descriptions = [data[title].get("description", "") for title in titles]

# Filter descriptions to ensure valid input
# In this case, more than 10 chars, and not None
valid_data = {
    title: meta for title, meta in data.items()
    if isinstance(meta.get("description"), str) and len(meta.get("description", "").strip()) > 10
}

titles = list(valid_data.keys())
descriptions = [valid_data[title]["description"] for title in titles]
print(f"Filtered down to {len(descriptions)} valid descriptions.")

# here is where we get the embedings by just passing through
# the description texts of each podcast
embeddings = bert_embed(descriptions)

"""Shira, the next one is just to save the embedings with the rest of the data from the .json in to the .csv file. So you probably just need to append them to your exsisting .csv file. Let me know if you have any questions. I also used LLMs to help figure some of this out and used BERT from huggingface - https://huggingface.co/docs/transformers/model_doc/bert, so I hope that is fine!"""

import csv
import numpy as np

# Paths for saving
csv_output_path = "/content/drive/MyDrive/podcast_data_with_bert_embeddings.csv"
embeddings_output_path = "/content/drive/MyDrive/podcast_description_bert_embeddings.npy"

# Save embeddings matrix to .npy
np.save(embeddings_output_path, embeddings)
print(f"Saved embeddings array to: {embeddings_output_path}")

# Write data to CSV
with open(csv_output_path, "w", newline='', encoding="utf-8") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["title", "description", "total_episodes", "embedding"])

    for title, desc, emb in zip(titles, descriptions, embeddings):
        meta = data.get(title, {})  # safely access metadata dict for this title
        episode_count = meta.get("total_episodes", "")

        writer.writerow([title, desc, episode_count, emb.tolist()])

print(f"CSV with embeddings and metadata created: {csv_output_path}")