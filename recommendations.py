# -*- coding: utf-8 -*-
"""Recommendations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UaXpEW3BfpcCZycQkAlt3TD2Pz81U293

Embedings data to recommendations

In this notebook we will take in .csv of book and podcast data. By giving a title within this .csv we can find the nearest books by calculating their nearest neighbors from the embedings. Hopefully we will add on more weights for the other factors within the .csv.
"""

# Load book & podcast data
import pandas as pd
import numpy as np
import ast

book_url='https://drive.google.com/file/d/1zDpeQRq4UMKoM83f958mxIK4hxdYodgH/view?usp=sharing'
book_url='https://drive.google.com/uc?id=' + book_url.split('/')[-2]
book_df = pd.read_csv(book_url)
book_df.drop(columns=['title'], inplace=True)
book_df.rename(columns={"clean_title": "title"}, inplace=True)
book_df['embeddings'] = book_df['embeddings'].apply(lambda x: np.array(ast.literal_eval(x)))

podcast_url = 'https://drive.google.com/file/d/10T5dTDKJfcMiIdPsgT4ySr73LKRn0RLj/view?usp=sharing'
podcast_url='https://drive.google.com/uc?id=' + podcast_url.split('/')[-2]
podcast_df = pd.read_csv(podcast_url)
podcast_df.rename(columns={"embedding": "embeddings"}, inplace=True)
podcast_df['embeddings'] = podcast_df['embeddings'].apply(lambda x: np.array(ast.literal_eval(x)))

book_df.head(10)

podcast_df.head(10)

import numpy as np
from scipy.spatial.distance import cosine

# First distance measure: cosine similarity
def cosine_similarity(vec1, vec2):
  dot_product = np.dot(vec1, vec2)
  norm_a = np.linalg.norm(vec1)
  norm_b = np.linalg.norm(vec2)
  return dot_product / (norm_a * norm_b)

# Second distance measure: euclidean distance
def euclidean_distance(vec1, vec2):
  vec1 = np.array(vec1)
  vec2 = np.array(vec2)
  return np.linalg.norm(vec1 - vec2)

# Get index of the input title
def title_to_num(title, dataframe):
  for i in range(len(dataframe)):
    if dataframe.iloc[i]['title'] == title:
      return i
  return -1

# Find the top x recommendations given either the item index or an embedding
def top_x_closest(dataframe, x, distance_measure, item_index=None, embedding=None, extra_param_weight=None):
  distances = {}
  if item_index is not None:
    vec_1 = dataframe.iloc[item_index]['embeddings']
  elif embedding is not None:
    vec_1 = embedding
  else:
    raise ValueError("Must provide either item_index or embedding")

  for i in range(len(dataframe)):
    if i != item_index:
      vec_2 = dataframe.iloc[i]['embeddings']
      distances[i] = distance_measure(vec_1, vec_2)

  is_similarity = None

  if distance_measure == cosine_similarity:
    is_similarity = True
  elif distance_measure == euclidean_distance:
    is_similarity = False

  if is_similarity is None:
    raise ValueError("Invalid distance measure")

  if extra_param_weight is not None:
    param = extra_param_weight[0]
    weight = extra_param_weight[1]
    target_val = extra_param_weight[2]
    for i in range(len(dataframe)):
      if i != item_index:
        distances[i] += weight * abs(target_val - dataframe.iloc[i][param])/10

  distances_sorted = sorted(
      distances.items(),
      key=lambda x: x[1],
      reverse=is_similarity      # True for cosine, False for euclidean
  )
  #distances_sorted = sorted(distances.items(), key=lambda x: x[1], reverse=True)
  return distances_sorted[:x]

from transformers import BertTokenizer, BertModel
import torch
import numpy as np

# Load model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Generate embedding based on an input description
def description_to_embedding(description, model, max_length=256):
  if not isinstance(description, str):
    raise ValueError("Description must be a string")
  elif len(description) == 0:
    raise ValueError("Description must not be empty")
  elif len(description) > max_length:
    raise ValueError(f"Description must be less than {max_length} characters")

  model.eval()
  with torch.no_grad():
    batch = [description]
    inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
    outputs = model(**inputs)
    last_hidden_state = outputs.last_hidden_state
    mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()

    # Mean pooling
    pooled_embedding = torch.sum(last_hidden_state * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)

  return pooled_embedding.cpu().numpy()

import textwrap

def get_recommendations():

  rec_type = ""
  input_type = ""
  x = 0
  answer = ""
  weight = -1
  target_val = -1

  print("Please enter \"podcast\" to get podcast recommendations and \"book\" to get book recommendations.")
  while rec_type != "podcast" and rec_type != "book":
    rec_type = input()
    if rec_type == "podcast":
      dataframe = podcast_df
      distance_measure = cosine_similarity
    elif rec_type == "book":
      dataframe = book_df
      distance_measure = euclidean_distance
    else:
      print("Please enter \"podcast\" or \"book\".")
      continue

  print(f"Please enter \"title\" to get recommendations based on a {rec_type} title from our library and \"description\" to get recommendations based on any {rec_type} description.")
  while input_type != "title" and input_type != "description":
    input_type = input()
    if input_type == "title":
      item_index = -1
      while item_index == -1:
        title = input("Please enter a title: ")
        item_index = title_to_num(title, dataframe)
        if item_index == -1:
          print("That title is not in our library, please try again.")
          continue
    elif input_type == "description":
      embedding = None
      while embedding is None:
        description = input("Please enter a description: ")
        try:
          embedding = description_to_embedding(description, model)
        except ValueError as e:
          print(e)
          continue
    else:
      print("Please enter \"title\" or \"description\".")
      continue

  #CODE FOR ADDITIONAL PARAMETERS AND WEIGHTS HERE
  if rec_type == "book":
    parameter = "first_publish_year"
  if rec_type == "podcast":
    parameter = "total_episodes"

  print(f"Would you like to add a weight for the additional parameter of {parameter}? Please type \"yes\" or \"no\". ")
  while answer != "yes" and answer != "no":
    answer = input()
    if answer == "yes":
      print(f"Please enter your target value for {parameter}.")
      while target_val < 0:
        target_val = int(input())
        if target_val < 0:
          print("Please enter a target value greater than 0.")
          continue
      print(f"Enter the weight you would like to give to {parameter} as a decimal.")
      while weight < 0 or weight > 1:
        weight = float(input())
        if weight < 0 or weight > 1:
          print("Please enter a weight between 0 and 1.")
    elif answer == "no":
      break
    else:
      print("Please enter \"yes\" or \"no\".")
      continue

  if answer == "yes":
    extra_param_weight = [parameter, weight, target_val]
  else:
    extra_param_weight = None

  print("Please enter the number of recommendations you would like, between 1 and 50: ")
  while x < 1 or x > 50:
    x = int(input())
    if x < 1 or x > 50:
      print("Please enter a number between 1 and 50.")
      continue

  print("Great! We are generating your recommendations...")

  if input_type == "title":
    recommendations = top_x_closest(dataframe, x, distance_measure, item_index=item_index, extra_param_weight=extra_param_weight)
  elif input_type == "description":
    recommendations = top_x_closest(dataframe, x, distance_measure, embedding=embedding, extra_param_weight=extra_param_weight)

  print("Here are your recommendations:")
  for i in range(x):
    print(f"\n----------------- {i+1} -----------------\n")
    print("Title: ", dataframe.iloc[recommendations[i][0]]['title'], "\n")
    print("Description: ", textwrap.fill(dataframe.iloc[recommendations[i][0]]['description'], width=75), "\n")
    if rec_type == "podcast":
      print("Number of episodes: ", dataframe.iloc[recommendations[i][0]]['total_episodes'], "\n")
    elif rec_type == "book":
      print("Author: ", dataframe.iloc[recommendations[i][0]]['author'], "\n")
      print("Year: ", int(dataframe.iloc[recommendations[i][0]]['first_publish_year']), "\n")
      print("Rating: ", dataframe.iloc[recommendations[i][0]]['rating'], "from", dataframe.iloc[recommendations[i][0]]['ratings_count'], "reviews on Goodreads!\n")

get_recommendations()

"""# Here is what is next:

The podcasts have the number of episodes that we can also weight.

The books have more features to weight. They are: year, ...


We also want to be able to input a new book or podcast description. Then turn those into embedings and then run the distances on them. I think the first test methods have you inputing the number in the csv, but instead we need to have it input the distance vector itself to use.

Also, we should change it so that we can take an title, find it, and then put its vector through the method for the more simple one. Maybe have a "Put in the title" and then run it, and if it isn't there a "Put in the description, not yet in catalouge". Should we add descriptions as we go?

# Update for Class

What we have:
- Podcast & Book Data and their embeddings in a format that is easily accessible to us
- Not as much podcast metadata as we would like, but just moving forward
- Two similarity algorithms to run on the embedings
- For a given title we can pull the top 5 recommendations
- Can give a description and get top 5 recommendations
- Adding in aditional features to the distances, therefore recommendations
- Getting all the functions to work together in a simple user interface

What we are working on:
- Running experiments on the two similarity algorithms
- Testing our recommendations for personal use & against common recs


When asking for a Book/Podcast year, prompt to add in additional parameters/requests/weights, and if they don't put in, then disreguard.
"""

# Do the tests for 2 similarity equations
# Will produce a graph
# By increasing the size of the dataset, get a curve to see how long they take


# Testing time it takes to run each (both are quick I think)
# but we can talk if we wanted to scale up

# Goodness of fit - maybe 3 podcasts and 3 books
# find top online recs and see how they compare?
# but that also goes against this whole idea if we don't like those
# we can do personal ones --> which sound better...?
# super subjective, but something

# Tests for weights
# Will produce graph

# Find at what point the recs are seemingly random
# because too much weight on parameter

import time
from matplotlib import pyplot as plt
import numpy as np

def benchmark_distance_methods(df, item_index=0, trials=20, k=10):
    cosine_times = []
    euclidean_times = []

    for _ in range(trials):
        # Cosine
        t0 = time.time()
        top_x_closest(df, k, cosine_similarity, item_index=item_index)
        cosine_times.append(time.time() - t0)

        # Euclidean
        t0 = time.time()
        top_x_closest(df, k, euclidean_distance, item_index=item_index)
        euclidean_times.append(time.time() - t0)

    return cosine_times, euclidean_times


def plot_speed_results(cos_times, eu_times):
    plt.figure(figsize=(8,4))
    plt.plot(cos_times, label="Cosine similarity")
    plt.plot(eu_times, label="Euclidean distance")
    plt.xlabel("Trial")
    plt.ylabel("Time (seconds)")
    plt.title("Runtime of cosine vs euclidean distance")
    plt.legend()
    plt.tight_layout()
    plt.show()

def rank_overlap(list1, list2):
    s1 = set([x[0] for x in list1])
    s2 = set([x[0] for x in list2])
    return len(s1.intersection(s2)) / len(s1)


def test_weight_sensitivity(df, distance_measure, param, item_index=0, k=10):
    weights = np.linspace(0, 1, 11)  # weights from 0.0 to 1.0
    overlaps = []

    # Baseline (no weight)
    baseline = top_x_closest(df, k, distance_measure, item_index=item_index)

    target_val = df.iloc[item_index][param]

    for w in weights:
        weighted = top_x_closest(
            df, k, distance_measure,
            item_index=item_index,
            extra_param_weight=[param, w, target_val]
        )
        overlap = rank_overlap(baseline, weighted)
        overlaps.append(overlap)

    return weights, overlaps


def plot_weight_sensitivity(weights, overlaps, param):
    plt.figure(figsize=(8,4))
    plt.plot(weights, overlaps, marker="o")
    plt.xlabel("Weight value")
    plt.ylabel("Rank Overlap with baseline")
    plt.title(f"Effect of weight on recommendations ({param})")
    plt.ylim(0,1)
    plt.tight_layout()
    plt.show()

cos_times, eu_times = benchmark_distance_methods(podcast_df, item_index=10)
plot_speed_results(cos_times, eu_times)

weights, overlaps = test_weight_sensitivity(
    podcast_df,
    cosine_similarity,
    param="total_episodes",
    item_index=10
)

plot_weight_sensitivity(weights, overlaps, "total_episodes")

weights, overlaps = test_weight_sensitivity(
    book_df,
    euclidean_distance,
    param="first_publish_year",
    item_index=5
)

plot_weight_sensitivity(weights, overlaps, "first_publish_year")