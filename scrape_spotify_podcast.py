# -*- coding: utf-8 -*-
"""Scrape_Spotify_Podcast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4Jelf2pnamer5r60yRjIaePLrpUx_I4

Getting Podcast Data using Spotify API
"""

pip install spotipy

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

#ADD PERSONAL CLIENT ID AND SECRET

#This cell gets show titles by searching for things and then just taking them.

#when I use this one, I get ~500 shows
def get_shows_from_query(query, limit=50):
    results = sp.search(q=query, type="show", limit=limit)
    shows = results["shows"]["items"]
    return [show["name"] for show in shows]

#this one can return ~4500 shows
def search_shows(query, pages=10):
    titles = []
    for i in range(pages):
        results = sp.search(
            q=query,
            type="show",
            limit=50,
            offset=i * 50
        )
        for show in results["shows"]["items"]:
            titles.append(show["name"])
    return titles

#just using random key works to try to get diverse range of podcast hits
queries = [
    "a", "the", "podcast", "show",
    "news", "crime", "history", "science",
    "life", "business", "sports", "tech"
]

titles = set()

for q in queries:
    for title in search_shows(q):
        titles.add(title)

print(f"Found {len(titles)} unique podcast titles.")
print(list(titles)[:20])

top_podcasts = titles

from google.colab import drive
import os

# Mount Drive
drive.mount('/content/drive')

# Create folder if it doesn't exist
SAVE_FOLDER = "/content/drive/My Drive"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# Save path
SAVE_FILE = os.path.join(SAVE_FOLDER, "spotify_podcast_data.json")

# Here we take the titles (that were gotten from SPotify, therefore should all be there)
# And use the API to grab the description and the number of episodes (I tried to get the ave length of the shows, but that took a lot of time)

import base64
import requests
import json
import time
import os
from tqdm import tqdm

# ========== CONFIGURATION ==========
CLIENT_ID = "your_client_id_here"
CLIENT_SECRET = "your_client_secret_here"

SAVE_FILE = "/content/drive/MyDrive/spotify_podcast_data.json"
BATCH_SIZE = 100
RETRY_LIMIT = 5
RETRY_DELAY = 2
THROTTLE_DELAY = 0.2

BASE_URL = "https://api.spotify.com/v1"

# ========== AUTHENTICATION (CLIENT CREDENTIALS FLOW) ==========
def get_spotify_token(client_id, client_secret):
    """Retrieve an access token using client credentials."""
    auth_str = f"{client_id}:{client_secret}"
    auth_bytes = base64.b64encode(auth_str.encode())
    headers = {"Authorization": f"Basic {auth_bytes.decode()}"}
    data = {"grant_type": "client_credentials"}

    res = requests.post("https://accounts.spotify.com/api/token", headers=headers, data=data)
    if res.status_code == 200:
        return res.json()["access_token"]
    else:
        raise Exception(f"Failed to authenticate: {res.text}")

# Get the token
token = get_spotify_token(client_id, client_secret)
headers = {"Authorization": f"Bearer {token}"}

# ========== SPOTIFY API HELPERS ==========
def safe_request(url, params=None, headers=headers):
    """Requests with retry logic"""
    for attempt in range(RETRY_LIMIT):
        try:
            r = requests.get(url, headers=headers, params=params)
            if r.status_code == 200:
                return r.json()
            elif r.status_code == 429:
                delay = int(r.headers.get("Retry-After", 10))
                print(f"Rate limited. Sleeping for {delay}s...")
                time.sleep(delay)
            else:
                print(f"Unexpected response {r.status_code}: {r.text}")
        except Exception as e:
            print(f"Error: {e}")

        time.sleep(RETRY_DELAY)
    return None

def search_podcast_by_title(title):
    url = f"{BASE_URL}/search"
    params = {"q": title, "type": "show", "limit": 1}
    return safe_request(url, params)

def get_show_details(show_id):
    url = f"{BASE_URL}/shows/{show_id}"
    return safe_request(url)

# ========== MAIN SCRAPER FUNCTION ==========
def fetch_podcast_metadata(titles):
    # Load or initialize data store
    if os.path.exists(SAVE_FILE):
        with open(SAVE_FILE, "r") as f:
            podcast_data = json.load(f)
        print(f"Loaded existing data: {len(podcast_data)} entries.")
    else:
        podcast_data = {}

    # Iterate over titles
    for i, title in enumerate(tqdm(titles)):
        if title in podcast_data:
            continue

        # Fetch podcast details
        search_res = search_podcast_by_title(title)
        if search_res and search_res["shows"]["items"]:
            show = search_res["shows"]["items"][0]
            show_id = show["id"]
            time.sleep(THROTTLE_DELAY)

            details = get_show_details(show_id)
            if details:
                podcast_data[title] = {
                    "description": details.get("description", "").strip(),
                    "total_episodes": details.get("total_episodes", 0),
                }
        else:
            podcast_data[title] = {"description": None, "total_episodes": None}

        # Save periodically
        if (i + 1) % BATCH_SIZE == 0:
            with open(SAVE_FILE, "w") as f:
                json.dump(podcast_data, f, indent=2)
            print(f"Saved progress: {i + 1} titles.")

    # Final save
    with open(SAVE_FILE, "w") as f:
        json.dump(podcast_data, f, indent=2)
    print("Scraping complete!")

# ========== RUN THE SCRAPER ==========
fetch_podcast_metadata(titles)

!pip install langdetect

import json
from langdetect import detect, LangDetectException

# Load the raw data
input_file = "/content/drive/My Drive/spotify_podcast_data.json"
output_file = "/content/drive/My Drive/spotify_podcast_data_cleaned.json"

# Load JSON
with open(input_file, "r") as f:
    data = json.load(f)

total_count = len(data)
cleaned_data = {}
removed_count = 0
error_count = 0
none_desc_count = 0

for title, metadata in data.items():
    try:
        title_lang = detect(title)

        description = metadata.get("description", "")
        if not isinstance(description, str):
            none_desc_count += 1
            description = ""   # Default to empty string if description is None

        desc_lang = detect(description) if description.strip() else ""  # Skip detection on empty

        # Keep entries where title and description (if present) are English
        if title_lang == "en" and (desc_lang in ["en", ""]):
            cleaned_data[title] = metadata
        else:
            removed_count += 1

    except LangDetectException:
        error_count += 1

# Save cleaned file
with open(output_file, "w") as f:
    json.dump(cleaned_data, f, indent=2)

kept_count = len(cleaned_data)

# Print summary
print(f"Total entries processed: {total_count}")
print(f"Removed (non-English or skipped): {removed_count}")
print(f"Skipped due to detection errors: {error_count}")
print(f"Entries with no description: {none_desc_count}")
print(f"Remaining (English only): {kept_count}")
print(f"✔️ Cleaned dataset saved to: {output_file}")

import json
from langdetect import detect, LangDetectException

# Load your raw data
input_file = "/content/drive/MyDrive/DIS Work/Core Course Work/podcast_data_small.json"  # <-- Update path if needed
output_file = "/content/drive/MyDrive/DIS Work/Core Course Work/podcast_data_small_cleaned.json"

# Load JSON
with open(input_file, "r") as f:
    data = json.load(f)

total_count = len(data)
cleaned_data = {}
removed_count = 0
error_count = 0
none_desc_count = 0

for title, metadata in data.items():
    try:
        title_lang = detect(title)

        description = metadata.get("description", "")
        if not isinstance(description, str):
            none_desc_count += 1
            description = ""   # Default to empty string if description is None

        desc_lang = detect(description) if description.strip() else ""  # Skip detection on empty

        # Keep entries where title and description (if present) are English
        if title_lang == "en" and (desc_lang in ["en", ""]):
            cleaned_data[title] = metadata
        else:
            removed_count += 1

    except LangDetectException:
        error_count += 1

# Save cleaned file
with open(output_file, "w") as f:
    json.dump(cleaned_data, f, indent=2)

kept_count = len(cleaned_data)

# Print summary
print(f"Total entries processed: {total_count}")
print(f"Removed (non-English or skipped): {removed_count}")
print(f"Skipped due to detection errors: {error_count}")
print(f"Entries with no description: {none_desc_count}")
print(f"Remaining (English only): {kept_count}")
print(f"✔️ Cleaned dataset saved to: {output_file}")