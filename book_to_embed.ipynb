{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#takes in the text and returns the embedings\n",
        "def bert_embed(texts, batch_size=16, max_length=256):\n",
        "    #Generate BERT embeddings for a list of texts using mean pooling.\n",
        "\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #for each text\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "\n",
        "            # Mean pooling\n",
        "            pooled_embedding = torch.sum(last_hidden_state * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "            embeddings.append(pooled_embedding.cpu().numpy())\n",
        "\n",
        "    return np.vstack(embeddings)\n"
      ],
      "metadata": {
        "id": "Iuv_rdlFcsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3xzQMuKpBGXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# combine data from each decades csv\n",
        "for decade in range(1900, 2021, 10):\n",
        "    csv_path = f\"/content/drive/My Drive/new_book_data/books_data_{decade}.csv\"\n",
        "    csv_data = pd.read_csv(csv_path)\n",
        "    all_data = pd.concat([all_data, csv_data], ignore_index=True)\n",
        "\n",
        "# filter data to remove instances of missing descriptions or descriptions of length < 10 characters\n",
        "all_data = all_data.dropna(subset=['description'])\n",
        "all_data = all_data[all_data['description'].str.len() > 10]\n",
        "\n",
        "# Remove text after \"----------\\nAlso contained in:\" if it's in the description\n",
        "all_data['description'] = all_data['description'].apply(lambda x: x.split(\"----------\\nAlso contained in:\")[0])"
      ],
      "metadata": {
        "id": "BthTOUbQRhEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = all_data['title'].values.tolist()\n",
        "descriptions = all_data['description'].values.tolist()\n",
        "\n",
        "# here is where we get the embedings by just passing through\n",
        "# the description texts of each podcast\n",
        "embeddings = bert_embed(descriptions)"
      ],
      "metadata": {
        "id": "CbZoIA8lcx4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_output_path = \"/content/drive/MyDrive/new_book_data/book_description_bert_embeddings.npy\"\n",
        "\n",
        "# Save embeddings matrix to .npy\n",
        "np.save(embeddings_output_path, embeddings)\n",
        "print(f\"Saved embeddings array to: {embeddings_output_path}\")\n",
        "\n",
        "# add column with embeddings to all_data and save to csv\n",
        "all_data['embeddings'] = embeddings.tolist()\n",
        "all_data.to_csv('/content/drive/My Drive/new_book_data/all_book_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6cvdFeQc7pZ",
        "outputId": "2855dbfa-bd03-4544-d075-4214c1eb7b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings array to: /content/drive/MyDrive/new_book_data/book_description_bert_embeddings.npy\n"
          ]
        }
      ]
    }
  ]
}